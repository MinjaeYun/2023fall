{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ddbc1d",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1X2ArcwjNDjeALhCAUKAIo_pSWbxZIkAG?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below to install package\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07452bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9f879a",
   "metadata": {},
   "source": [
    "# Reading and Preprocessing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba7831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "nlsy = pd.read_csv('https://github.com/Mixtape-Sessions/Machine-Learning/blob/main/Labs/data/nlsy97.csv?raw=true')\n",
    "\n",
    "# Generate dictionary of transformations of education\n",
    "powerlist = [nlsy['educ']**j for j in np.arange(2, 5)]\n",
    "X = pd.concat(powerlist, axis=1)\n",
    "X.columns = ['educ' + str(j) for j in np.arange(2, 5)]\n",
    "\n",
    "print(len(nlsy.columns))\n",
    "nlsy = pd.concat([nlsy,X],axis=1)\n",
    "print(len(nlsy.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759078c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data frame\n",
    "# Drop rows with missing values\n",
    "# Always check documentation\n",
    "print(len(nlsy))\n",
    "condition = nlsy['cv_bio_mom_age_child1_1997'] < 0\n",
    "nlsy_cleaned = nlsy.drop(nlsy[condition].index)\n",
    "print(len(nlsy_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808793a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize X's\n",
    "X = nlsy.drop(columns=['lnw_2016','mom_educCOLL'])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b8283",
   "metadata": {},
   "source": [
    "# Pre-Analysis Decision\n",
    "- Defining the choice set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can you identify variables with seemingly dummy/categorical/discrete variables?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4157f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the list of columns in the cols object\n",
    "cols = nlsy.columns.to_list() # method 1\n",
    "cols = list(nlsy.columns) # method 2\n",
    "cols = [x for x in cols if x!=\"lnw_2016\"]\n",
    "cols[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60fbd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find unique values\n",
    "list(set(nlsy['black'])) # method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90e83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlsy['black'].unique() # method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534f0972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Identify variables with more than three variations\n",
    "continuous_var = []\n",
    "for col in cols:\n",
    "    if len(list(set(nlsy[col]))) > 3:\n",
    "        continuous_var.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b26466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# , for assigning multiple values at once \n",
    "e1, e2 = 1, 2\n",
    "print(e1)\n",
    "print(e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively\n",
    "e1 = 1\n",
    "e2 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f225e31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of subplots for histograms\n",
    "fig,(ax11,ax21) = plt.subplots(2,1,figsize=(12, 4))\n",
    "ax11.hist(nlsy[continuous_var[4]], bins=20, color='blue')\n",
    "ax11.set_title(f'{continuous_var[4]}')\n",
    "fig.subplots_adjust(hspace=1) \n",
    "ax21.hist(nlsy[continuous_var[5]], bins=20, color='blue')\n",
    "ax21.set_title(f'{continuous_var[5]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ae40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs= plt.subplots(2,2,figsize=(12, 4)) # Always assign an empty array first\n",
    "axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06931a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of subplots for histograms\n",
    "fig,axs= plt.subplots(2,2,figsize=(12, 4))\n",
    "axs[0,0].hist(nlsy[continuous_var[4]], bins=20, color='blue')\n",
    "axs[0,0].set_title(f'{continuous_var[4]}')\n",
    "\n",
    "axs[1,0].hist(nlsy[continuous_var[5]], bins=20, color='blue')\n",
    "axs[1,0].set_title(f'{continuous_var[5]}')\n",
    "fig.subplots_adjust(hspace=1) \n",
    "axs[0,1].hist(nlsy[continuous_var[6]], bins=20, color='blue')\n",
    "axs[0,1].set_title(f'{continuous_var[6]}')\n",
    "\n",
    "axs[1,1].hist(nlsy[continuous_var[7]], bins=20, color='blue')\n",
    "axs[1,1].set_title(f'{continuous_var[7]}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a588ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of continuous_var\n",
    "len(continuous_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4928cd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequentially assign element into a matrix\n",
    "matrix = np.array(continuous_var).reshape(10, 9) # 10 rows and 9 columns\n",
    "matrix[3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ee869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index of an element in a list\n",
    "continuous_var.index('pc9_001_1997') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc5db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relationship to matrix\n",
    "continuous_var.index('pc9_001_1997') == 9*3 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afcf7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Create a grid of subplots for histograms of all continuous_var\n",
    "# Use for loop for iterating the rows and columns\n",
    "fig, axs = plt.subplots(10, 9, figsize=(10, 10))\n",
    "matrix = np.array(continuous_var).reshape(10, 9)\n",
    "\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fde126",
   "metadata": {},
   "source": [
    "# Linear Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074931dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit the linear regression model\n",
    "reg = linear_model.LinearRegression().fit(X_scaled, nlsy['lnw_2016'])\n",
    "\n",
    "# Generate predicted values\n",
    "yhat = reg.predict(X_scaled)\n",
    "print(len(yhat),len(X_scaled),len(nlsy))\n",
    "yhat[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb17f7ff",
   "metadata": {},
   "source": [
    "# Lasso Regression Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=nlsy[continuous_var].drop(columns=['exp'])\n",
    "\n",
    "# Divide into training and test set so we can honestly gauge predictive accuracy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, nlsy['lnw_2016'],random_state=42)\n",
    "# Scale regressors\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Do cross-validated Lasso (the easy way!)\n",
    "# instantiate and fit our lassocv object\n",
    "alphas = (.0001,.001,.002, .004, .006, .008, .01, .012, .014, .016 ,.018, .02)\n",
    "lassocv=linear_model.LassoCV(cv=5,alphas=alphas,random_state=42).fit(X_train_scaled,y_train)\n",
    "\n",
    "# print out the chosen value for alpha\n",
    "print(\"Chosen alpha: {:.3f}\".format(lassocv.alpha_))\n",
    "# print the original number of regressors and the number selected by lasso\n",
    "print(\"Number of regressors in the menu: \",len(X.columns))\n",
    "print(\"Number of regressors selected by lasso: \",sum(lassocv.coef_!=0))\n",
    "# print out accuracy on training and test test\n",
    "print(\"Accuracy on training set: {:.3f}\".format(lassocv.score(X_train_scaled,y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(lassocv.score(X_test_scaled,y_test)))\n",
    "# look at the coefficients\n",
    "results = pd.DataFrame({'feature': X.columns[lassocv.coef_!=0],'coefficient': lassocv.coef_[lassocv.coef_!=0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e6813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on covariates with continuous variations \n",
    "X=nlsy[continuous_var].drop(columns=['exp'])\n",
    "# Divide into training and test set so we can honestly gauge predictive accuracy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, nlsy['lnw_2016'],random_state=42)\n",
    "# Scale regressors\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "ridgecv=linear_model.RidgeCV(cv=5,alphas=(.1,1,10,50,100,1000)).fit(X_train_scaled,y_train)\n",
    "print(\"Chosen alpha: {:.3f}\".format(ridgecv.alpha_))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(ridgecv.score(X_train_scaled,y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(ridgecv.score(X_test_scaled,y_test)))\n",
    "# look at the coefficients\n",
    "results = pd.DataFrame({'feature': X.columns[ridgecv.coef_!=0],'coefficient': ridgecv.coef_[ridgecv.coef_!=0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7778682",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ridgecv.coef_, 's', label=\"RidgeCV\")\n",
    "plt.plot(lassocv.coef_, 'o', label=\"LassoCV\")\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "xlims = plt.xlim()\n",
    "plt.hlines(0, xlims[0], xlims[1])\n",
    "plt.xlim(xlims)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bf29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall the example in Lecture 3\n",
    "X=nlsy.drop(columns=['exp'])\n",
    "\n",
    "# Divide into training and test set so we can honestly gauge predictive accuracy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, nlsy['lnw_2016'],random_state=42)\n",
    "# Scale regressors\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# Do cross-validated Lasso (the easy way!)\n",
    "# instantiate and fit our lassocv object\n",
    "alphas = (.001,.002, .004, .006, .008, .01, .012, .014, .016 ,.018, .02)\n",
    "lassocv=linear_model.LassoCV(cv=5,alphas=alphas,random_state=42).fit(X_train_scaled,y_train)\n",
    "\n",
    "# print out the chosen value for alpha\n",
    "print(\"LASSO \\n Chosen alpha: {:.3f}\".format(lassocv.alpha_))\n",
    "# print the original number of regressors and the number selected by lasso\n",
    "print(\"Number of regressors in the menu: \",len(X.columns))\n",
    "print(\"Number of regressors selected by lasso: \",sum(lassocv.coef_!=0))\n",
    "# print out accuracy on training and test test\n",
    "print(\"Accuracy on training set: {:.3f}\".format(lassocv.score(X_train_scaled,y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(lassocv.score(X_test_scaled,y_test)))\n",
    "# look at the coefficients\n",
    "results = pd.DataFrame({'feature': X.columns[lassocv.coef_!=0],'coefficient': lassocv.coef_[lassocv.coef_!=0]})\n",
    "\n",
    "# Focus on continuous variations\n",
    "X=nlsy.drop(columns=['lnw_2016','exp'])\n",
    "# Divide into training and test set so we can honestly gauge predictive accuracy\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, nlsy['lnw_2016'],random_state=42)\n",
    "# Scale regressors\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "ridgecv=linear_model.RidgeCV(cv=5,alphas=(.1,1,10,50,100,1000)).fit(X_train_scaled,y_train)\n",
    "print(\"Ridge \\n Chosen alpha: {:.3f}\".format(ridgecv.alpha_))\n",
    "print(\"Accuracy on training set: {:.3f}\".format(ridgecv.score(X_train_scaled,y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(ridgecv.score(X_test_scaled,y_test)))\n",
    "# look at the coefficients\n",
    "results = pd.DataFrame({'feature': X.columns[ridgecv.coef_!=0],'coefficient': ridgecv.coef_[ridgecv.coef_!=0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924931b3",
   "metadata": {},
   "source": [
    "# Loop through Pre-Analysis Decision\n",
    "\n",
    "- Finding the best threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab04828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tentative threshold \"3\" could be iteratively evaluated\n",
    "continuous_var = []\n",
    "for col in nlsy.columns.to_list():\n",
    "    if  len(list(set(nlsy[col]))) > 3:\n",
    "        continuous_var.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70b4781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Find the best threshold of variations \n",
    "# Get an empty list named as \"collect_diff\"\n",
    "# Get a range from 2 - 4 names as \"range_of_value\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c40fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Loop through the accuracy model\n",
    "for variation in range_of_value: \n",
    "    # Get the new list of continuous variables\n",
    "    continuous_var = []\n",
    "            # ...\n",
    "    # Define  X, and y\n",
    "\n",
    "\n",
    "    # Divide into training and test set so we can honestly gauge predictive accuracy\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, nlsy['lnw_2016'],random_state=42)\n",
    "    \n",
    "    # Scale regressors\n",
    "        # ...\n",
    "        \n",
    "    # Run LassoCV\n",
    "        # ...\n",
    "        \n",
    "    # Print accuracy on train data \n",
    "        # ...\n",
    "    \n",
    "    # Append the difference between accuracy in the list\n",
    "    collect_diff.append(lassocv.score(X_train_scaled,y_train)-lassocv.score(X_test_scaled,y_test))\n",
    "\n",
    "print(f\"The minimum testing accuracy loss is {min(collect_diff)} \\n\"  + \n",
    "      f\"The threshold values for the minimum testing accuracy loss is {range_of_value[collect_diff.index(min(collect_diff))]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b9116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the \"best\" choice set based on the results above\n",
    "cols = [x for x in nlsy.columns.to_list() if x!=\"lnw_2016\"]\n",
    "\n",
    "continuous_var = []\n",
    "for col in cols:\n",
    "    if  len(list(set(nlsy[col]))) >range_of_value[collect_diff.index(min(collect_diff))]:\n",
    "        continuous_var.append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd81304",
   "metadata": {},
   "source": [
    "# Residualization\n",
    "1. Reg $\\tilde{Y}$ on $\\tilde{D}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523653c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get y, d, X's \n",
    "# Standardize\n",
    "y = nlsy['lnw_2016']\n",
    "d = nlsy['mom_educCOLL'] \n",
    "X = nlsy[continuous_var]\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceff36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Run LASSO for y and d\n",
    "\n",
    "# calculate residuals for y\n",
    "\n",
    "# Run the LassoCV\n",
    "\n",
    "# Calculate residuals d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c2dabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear projection of y tilde on d tilde\n",
    "d_reshaped = dtilde.values.reshape(-1,1) # 1 dimensional array needs reshaping for regression packages\n",
    "y_reshaped = ytilde.values.reshape(-1,1)\n",
    "lm=linear_model.LinearRegression()\n",
    "lm.fit(d_reshaped,y_reshaped)\n",
    "lm.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36fd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the standard deviations\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "\n",
    "# Fit the linear model\n",
    "d_reshaped = sm.add_constant(d_reshaped)\n",
    "model = sm.OLS(y_reshaped, d_reshaped).fit(cov_type='HC3') # Robust standard errors\n",
    "\n",
    "# Get the coefficients\n",
    "coefficients = model.params\n",
    "print(\"Coefficients:\", coefficients)\n",
    "\n",
    "# Get the standard errors\n",
    "standard_errors = model.bse\n",
    "print(\"Standard Errors:\", standard_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c645b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about a simple regression of Y on D\n",
    "# reshape the 1 dimensional array\n",
    "d_original = d.values.reshape(-1,1) \n",
    "d_original = sm.add_constant(d_original)\n",
    "model = sm.OLS(y.values.reshape(-1,1),d_original).fit(cov_type='HC3')\n",
    "\n",
    "# Get the coefficients\n",
    "coefficients = model.params\n",
    "print(\"Coefficients:\", coefficients)\n",
    "\n",
    "# Get the standard errors\n",
    "standard_errors = model.bse\n",
    "print(\"Standard Errors:\", standard_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85008c5",
   "metadata": {},
   "source": [
    "2. Reg $Y$ on $D$ and All $X$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec3b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# How about Y on D + X?\n",
    "# Define X and Y\n",
    "X = pd.concat([nlsy['mom_educCOLL'],X],axis=1)\n",
    "\n",
    "# Add constant\n",
    "\n",
    "# Run the linear model\n",
    "\n",
    "# Get the coefficients\n",
    "\n",
    "# Print the coefficients\n",
    "\n",
    "# Get the standard errors\n",
    "\n",
    "# Print the standard errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f9334",
   "metadata": {},
   "source": [
    "3. LASSO Reg $Y$ on $D$ and All $X$'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a027067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about LASSO with all variables?\n",
    "y = nlsy['lnw_2016']\n",
    "d = nlsy['mom_educCOLL'] \n",
    "X = nlsy.drop(columns=['lnw_2016','mom_educCOLL'])\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57077f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Run LassoCV on Y\n",
    "\n",
    "# calculate residual for y\n",
    "\n",
    "# Run LassoCV on D\n",
    "\n",
    "# Calculate residuals\n",
    "    # ...\n",
    "\n",
    "d_reshaped = dtilde.values.reshape(-1,1) # 1 dimensional array needs reshaping for regression packages\n",
    "y_reshaped = ytilde.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21907f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Fit the linear model\n",
    "d_reshaped = sm.add_constant(d_reshaped)\n",
    "model = sm.OLS(y_reshaped, d_reshaped).fit(cov_type='HC3')\n",
    "\n",
    "# Get the coefficients\n",
    "coefficients = model.params\n",
    "print(\"Coefficients:\", coefficients)\n",
    "\n",
    "# Get the standard errors\n",
    "standard_errors = model.bse\n",
    "print(\"Standard Errors:\", standard_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daafb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How about a simple regression with all variables?\n",
    "y = nlsy['lnw_2016']\n",
    "X = nlsy.drop(columns=['lnw_2016','mom_educCOLL'])\n",
    "d_original = pd.concat([nlsy['mom_educCOLL'],X],axis=1)\n",
    "d_original = sm.add_constant(d_original)\n",
    "model = sm.OLS(y.values.reshape(-1,1),d_original).fit(cov_type='HC3')\n",
    "# Get the coefficients\n",
    "coefficients = model.params\n",
    "print(\"Coefficients:\", coefficients['mom_educCOLL'])\n",
    "\n",
    "# Get the standard errors\n",
    "standard_errors = model.bse\n",
    "print(\"Standard Errors:\", standard_errors['mom_educCOLL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df538e4e",
   "metadata": {},
   "source": [
    "# K-fold cross validations by steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddc44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Assign variables\n",
    "y = nlsy['lnw_2016']\n",
    "d = nlsy['mom_educCOLL']\n",
    "X = nlsy[continuous_var].drop(columns=['lnw_2016'])\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2f3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look inside the k-fold\n",
    "[[x,y] for x,y in kf.split(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d47b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[[len(x),len(y)] for x,y in kf.split(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b102ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign alphas and results storage\n",
    "alphas = (.001, .002, .004, .006, .008, .01, .012, .014, .016, .018, .02)\n",
    "best_alpha_y = 0\n",
    "best_alpha_d = 0\n",
    "best_mse_y = float('inf') # Assign type of a null value capable of any minimum values\n",
    "best_mse_d = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(best_mse_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3767a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    d_train, d_test = d.iloc[train_index], d.iloc[test_index]\n",
    "    \n",
    "    # Loop through each fold\n",
    "    # ...\n",
    "        \n",
    "        # Fit Lasso for y\n",
    "        lasso_y = Lasso(alpha=alpha)\n",
    "        lasso_y.fit(X_train, y_train)\n",
    "        mse_y = mean_squared_error(y_test, lasso_y.predict(X_test))\n",
    "        \n",
    "        # Practice!\n",
    "        # Fit Lasso for d\n",
    "        # ...\n",
    "        \n",
    "        # Practice!\n",
    "        # Update best alpha and mse for y\n",
    "        # In other words, assign the new MSE \"mse_y\" to the best_mse_y\n",
    "        # If the current \"mse_y\" is the smallest\n",
    "        \n",
    "            \n",
    "        # Update best alpha and mse for d\n",
    "        # In other words, assign the new MSE \"mse_d\" to the best_mse_d\n",
    "        # If the current \"mse_d\" is the smallest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c764b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the final models with the best alphas\n",
    "lasso_y_final = Lasso(alpha=best_alpha_y).fit(X_scaled, y)\n",
    "lasso_d_final = Lasso(alpha=best_alpha_d).fit(X_scaled, d)\n",
    "\n",
    "# Calculate residuals\n",
    "ytilde = y - lasso_y_final.predict(X_scaled)\n",
    "dtilde = d - lasso_d_final.predict(X_scaled)\n",
    "\n",
    "# Your existing code for OLS regression with residuals can follow here\n",
    "d_reshaped = dtilde.values.reshape(-1,1) # 1 dimensional array needs reshaping for regression packages\n",
    "y_reshaped = ytilde.values.reshape(-1,1)\n",
    "\n",
    "d_reshaped = sm.add_constant(d_reshaped)\n",
    "model = sm.OLS(y_reshaped, d_reshaped).fit(cov_type='HC3')\n",
    "\n",
    "# Get the coefficients\n",
    "coefficients = model.params\n",
    "\n",
    "print(\"Coefficients: {:.3f}\".format(coefficients[1]))\n",
    "\n",
    "# Get the standard errors\n",
    "standard_errors = model.bse\n",
    "print(\"Standard Errors: {:.3f}\".format(standard_errors[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262b58f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# More on Sample Spliting \n",
    "Practice with Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad0b089",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% What if I want to separate out the training/testing sets based on race?\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame with 1266 observations and a 'race' column\n",
    "# Replace this with your actual DataFrame\n",
    "# Filter the DataFrame based on race\n",
    "nlsy_hispanic = nlsy[nlsy['hispanic'] == 1]\n",
    "nlsy_black = nlsy[nlsy['black'] == 1]\n",
    "condition = (nlsy['hispanic'] != 1)&(nlsy['black'] != 1)\n",
    "nlsy_other = nlsy[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9857559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does logical expression do?\n",
    "[False]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202bd41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "arbitrary = [False]*(150) + [True]*300 + [False]*(len(nlsy)-150-300)\n",
    "arbitrary[140], arbitrary[400], arbitrary[650]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb3286f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlsy[arbitrary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794e177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(nlsy.dad_educ)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697c6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train/test split for each racial group\n",
    "X_train_hispanic, X_test_hispanic, y_train_hispanic, y_test_hispanic = train_test_split(nlsy_hispanic.drop(['lnw_2016','mom_educCOLL'], axis=1), nlsy_hispanic['lnw_2016'], test_size=0.5, random_state=0)\n",
    "X_train_black, X_test_black, y_train_black, y_test_black = train_test_split(nlsy_black.drop(['lnw_2016','mom_educCOLL'], axis=1), nlsy_black['lnw_2016'], test_size=0.5, random_state=0)\n",
    "X_train_other, X_test_other, y_train_other, y_test_other = train_test_split(nlsy_other.drop(['lnw_2016','mom_educCOLL'], axis=1), nlsy_other['lnw_2016'], test_size=0.5, random_state=0)\n",
    "\n",
    "# Combine the train sets and test sets to get the final train and test sets\n",
    "X_train = pd.concat([X_train_hispanic, X_train_black, X_train_other])\n",
    "X_test = pd.concat([X_test_hispanic, X_test_black, X_test_other])\n",
    "y_train = pd.concat([y_train_hispanic, y_train_black, y_train_other])\n",
    "y_test = pd.concat([y_test_hispanic, y_test_black, y_test_other])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b340c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train),len(X_test),len(y_train),len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138362ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Perform train/test split for dad's education level (dad_educ)\n",
    "# Find the unique values from nlsy['dad_educ']\n",
    "# ...\n",
    "\n",
    "xtrains = []\n",
    "xtests = []\n",
    "ytrains = []\n",
    "ytests = []\n",
    "# Loop through unique values in nlsy['dad_educ']\n",
    "# Use \"condition\" to find the subset for each value in nlsy['dad_educ']\n",
    "# ...\n",
    "# ...\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(subset.drop(['lnw_2016'], axis=1), subset['lnw_2016'], test_size=0.5, random_state=0)\n",
    "    xtrains.append(xtrain)\n",
    "    xtests.append(xtest)\n",
    "    ytrains.append(ytrain)\n",
    "    ytests.append(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the train sets and test sets to get the final train and test sets\n",
    "X_train = pd.concat(xtrains)\n",
    "X_test = pd.concat(xtests)\n",
    "y_train = pd.concat(ytrains)\n",
    "y_test = pd.concat(ytests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6893d49",
   "metadata": {},
   "source": [
    "Now we can manually separate the training sets based on criteria tailered to each research question!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1418f0ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Post Double Selection Lasso\n",
    "- Belloni, Chernozhukov, Hansen\n",
    "\n",
    "**PDS is implemented in three steps:**\n",
    "1. Lasso $Y_i$ on $X_i$, collect retained features in $X_i^Y$\n",
    "2. Lasso $D_i$ on $X_i$, collect retained features in $X_i^D$\n",
    "3. Regress $Y_i$ on $D_i$ and $X_i^Y \\cup X_i^D$\n",
    "\n",
    "*Caveats and considerations:*\n",
    "- Standardizing controls pre-lasso is important\n",
    "- BCH have a formula for the penalty parameter, but cross-validation seems to work just fine\n",
    "- Inference: just use robust SEs from last step!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26f918",
   "metadata": {},
   "source": [
    "### Step 1: Lasso the outcome on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f0b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Assign values on y, d, X and standardize X\n",
    "# Run LassoCV of Y on X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what it looks like!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1969aacd",
   "metadata": {},
   "source": [
    "### Step 2: Lasso the treatment on X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97256d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what it looks like!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ee4773",
   "metadata": {},
   "source": [
    "### Step 3: Form the union of controls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd116ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xunion=X.iloc[:,(lassod.coef_!=0) + (lassoy.coef_!=0)]\n",
    "Xunion.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6df86c",
   "metadata": {},
   "source": [
    "### Concatenate treatment with union of controls and regress y on that and print out estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50e3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Get the all covariates and run the regression\n",
    "\n",
    "\n",
    "print(\"PDS regression earnings mom's college gap: {:.3f}\".format(fullreg.coef_[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff23b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36dbb12e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21437c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = nlsy.drop(columns=['lnw_2016','mom_educCOLL']) \n",
    "y = nlsy['lnw_2016']\n",
    "d = nlsy['mom_educCOLL']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3bc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First without cross-validating\n",
    "rf=RandomForestRegressor(random_state=42).fit(X_train,y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(rf.score(X_train,y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(rf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c093abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now with cross-validation\n",
    "# define grid for max_depth\n",
    "param_grid = {'max_depth': [5,10,100]}\n",
    "grid_searchrf = GridSearchCV(RandomForestRegressor(),param_grid,cv=5,return_train_score=True).fit(X_train,y_train)\n",
    "print(\"Best max_depth: \",grid_searchrf.best_estimator_.get_params()['max_depth'])\n",
    "print(\"Accuracy on training set: {:.3f}\".format(grid_searchrf.score(X_train,y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(grid_searchrf.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19232f74",
   "metadata": {},
   "source": [
    "# Residualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b351b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import numpy as np\n",
    "X = nlsy.drop(columns=['lnw_2016','mom_educCOLL']) \n",
    "y = nlsy['lnw_2016']\n",
    "d = nlsy['mom_educCOLL']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "d_train, d_test = d[X_train.index], d[X_test.index]\n",
    "\n",
    "# Create a Random Forest Classifier with max_depth=3\n",
    "rf_y = RandomForestRegressor(max_depth=3, random_state=42)\n",
    "rf_d = RandomForestRegressor(max_depth=3, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_y.fit(X_train, y_train)\n",
    "rf_d.fit(X_train, d_train)\n",
    "\n",
    "# Calculate residuals for y \n",
    "y_residual = y_test - rf_y.predict(X_test)\n",
    "\n",
    "# Calculate residuals for d on entire data\n",
    "d_residual = d_test - rf_d.predict(X_test)\n",
    "\n",
    "# Run a linear regression of the residualized y on the residualized d\n",
    "lr = LinearRegression()\n",
    "lr.fit(d_residual.values.reshape(-1, 1), y_residual.values.reshape(-1, 1))\n",
    "\n",
    "# Coefficient for residualized d\n",
    "print(\"Coefficient for residualized d:\", lr.coef_[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d56279",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(d_residual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8326f6fc",
   "metadata": {},
   "source": [
    "## Average K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ba10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "X = nlsy.drop(columns=['lnw_2016','mom_educCOLL']) \n",
    "y = nlsy['lnw_2016']\n",
    "d = nlsy['mom_educCOLL']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300657b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Initialize Random Forest Regressor\n",
    "\n",
    "# Initialize Linear Regression\n",
    "\n",
    "# Initialize k-fold cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ca77b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GridSearchCV\n",
    "grid_search_y = GridSearchCV(estimator=rf_y, param_grid=param_grid, cv=kf, n_jobs=-1)\n",
    "grid_search_d = GridSearchCV(estimator=rf_d, param_grid=param_grid, cv=kf, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3763eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Initialize storage for coefficients\n",
    "coefficients = []\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in kf.split(X_train):\n",
    "    # Practice!\n",
    "    # Split data into training and test sets (x,y,d)\n",
    "    # Use train_index and test_index\n",
    "    # ...\n",
    "\n",
    "    # Fit GridSearchCV for y and d\n",
    "    # ...\n",
    "\n",
    "    # Get the best estimator\n",
    "    # ...\n",
    "\n",
    "    # Calculate residuals for y on test data\n",
    "    # ...\n",
    "\n",
    "    # Calculate residuals for d on entire data\n",
    "    # ...\n",
    "\n",
    "    # Run a linear regression of the residualized y on the residualized d\n",
    "    # Don't forget to reshape!\n",
    "\n",
    "    \n",
    "    # Store coefficient for residualized d\n",
    "    coefficients.append(lr.coef_[0][0])\n",
    "\n",
    "# Average coefficient across all folds\n",
    "avg_coefficient = np.mean(coefficients)\n",
    "\n",
    "print(\"Average Coefficient for residualized d:\", avg_coefficient)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a54c7ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Double/de-biased machine learning (DML): Preliminaries\n",
    "- Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, Newey, Robins\n",
    "\n",
    "### DML strategy:\n",
    "\n",
    "1. Predict $Y_i$ on $X_i$ with ML and compute the residuals,\n",
    "$\\tilde{Y_i} = Y_i âˆ’ Y_i^{\\text{ML}}$, where\n",
    "$Y_i^{\\text{ML}} = $ prediction generated by ML\n",
    "\n",
    "2. Predict $D_i$ on $X_i$ with ML and compute the residuals,\n",
    "$\\tilde{D_i} = D_i - D_i^{\\text{ML}}$, where $D_i^{\\text{ML}} = $ prediction generated by ML\n",
    "\n",
    "3. Regress $\\tilde{Y_i}$ on $\\tilde{D_i}$.\n",
    "\n",
    "$Y_i^{\\text{DML}}$ and $D_i^{\\text{DML}}$ should be predictions generated by a machine learning model trained on a set of observations that does not include i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4be7ef",
   "metadata": {},
   "source": [
    "**Recipe:**\n",
    "\n",
    "1. Divide the sample into K folds\n",
    "2. For k = 1, .... ,K\n",
    "> - Train a model to predict $Y$ given $X$, leaving out observations i in fold k: $\\hat{Y}^{-k}(x)$ \n",
    "> - Train a model to predict D given X, leaving out observations i in fold k: $\\hat{D}^{-k}(x)$\n",
    "> - Form residuals $\\tilde{Y_i}$ = $Y_i$ - $\\hat{Y}^{-k}(x)$ and $\\tilde{D_i}$ = $D_i$ - $\\hat{D}^{-k}(x)$\n",
    "3. Regress $\\tilde{Y_i}$ on $\\tilde{D_i}$.\n",
    "\n",
    "**Caveats and considerations:**\n",
    "\n",
    "- Cross-validation to choose tuning parameters\n",
    "- Inference: use robust SEs from last step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ec6ead",
   "metadata": {},
   "source": [
    "For simplicity, we will first do it without sample splitting\n",
    "\n",
    "### Step 1: Ridge outcome on Xs, get residuals\n",
    "Try yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bac6303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variables\n",
    "\n",
    "# Standardize the features\n",
    "\n",
    "# Run RidgeCV of Y on Xs and get residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7a38c",
   "metadata": {},
   "source": [
    "### Step 2: Ridge treatment on Xs, get residuals\n",
    "Try yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34750405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RidgeCV of D on Xs and get residuals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4b674",
   "metadata": {},
   "source": [
    "### Step 3: Regress y resids on d resids and print out estimate\n",
    "Try yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17613a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dca5dbd3",
   "metadata": {},
   "source": [
    "### The real thing: with sample splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6277b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variables\n",
    "X = nlsy.drop(columns=['lnw_2016', 'mom_educCOLL'])  # Replace with your actual feature columns\n",
    "y = nlsy['lnw_2016']\n",
    "d = nlsy['mom_educCOLL']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# create our sample splitting \"object\"\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=42)\n",
    "\n",
    "# apply the splits to our Xs\n",
    "kf.get_n_splits(X)\n",
    "\n",
    "# initialize columns for residuals\n",
    "yresid = y*0\n",
    "dresid = d*0\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X_scaled, nlsy['lnw_2016'],random_state=42)\n",
    "X_scaled= pd.DataFrame(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b667e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now loop through each fold\n",
    "ii=0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X_scaled.iloc[train_index,:], X_scaled.iloc[test_index,:]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    d_train, d_test = d.iloc[train_index], d.iloc[test_index]\n",
    " \n",
    "    # Do DML thing\n",
    "    # Ridge y on training folds:\n",
    "    ridgey.fit(X_train, y_train)\n",
    "\n",
    "    # but get residuals in test set\n",
    "    yresid.iloc[test_index]=y_test-ridgey.predict(X_test)\n",
    "\n",
    "    #Ridge d on training folds\n",
    "    ridged.fit(X_train, d_train)\n",
    "\n",
    "    #but get residuals in test set\n",
    "    dresid.iloc[test_index]=d_test-ridged.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b28af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress resids\n",
    "dresid = dresid.values.reshape(-1,1)\n",
    "yresid = yresid.values.reshape(-1,1)\n",
    "dmlreg=linear_model.LinearRegression().fit(dresid,yresid)\n",
    "\n",
    "print(\"DML regression earnings mom's college gap: {:.3f}\".format(dmlreg.coef_[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3159bc",
   "metadata": {},
   "source": [
    "**What I hope you've gotten out of the last couple of days:**\n",
    "- Clarity on distinction between predictive and causal questions\n",
    "- Foot in the door with python implementations of some common modern supervised machine learning methods\n",
    "- Tools for using ML methods to control for high dimensional covariates in the service of causal inference\n",
    "\n",
    "**Preview for future workshops:**\n",
    "- Use ML to predict heterogeneous treatment effects (e.g., random causal forests)\n",
    "- ML and instrumental variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4ff0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10d24c4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define features and target variables\n",
    "X = nlsy[continuous_var]\n",
    "y = nlsy['lnw_2016']\n",
    "d = nlsy['mom_educCOLL']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize neural network\n",
    "nn = MLPRegressor(hidden_layer_sizes=(5,), max_iter=500, random_state=42)\n",
    "\n",
    "# Initialize variables to store residuals\n",
    "y_residuals = np.zeros(y.shape)\n",
    "d_residuals = np.zeros(d.shape)\n",
    "\n",
    "# Perform sample splitting\n",
    "for train_index, test_index in kf.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_index], X_scaled[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    d_train, d_test = d.iloc[train_index], d.iloc[test_index]\n",
    "    \n",
    "    # Train neural network for y\n",
    "    nn.fit(X_train, y_train)\n",
    "    y_pred = nn.predict(X_test)\n",
    "    y_residuals[test_index] = y_test - y_pred\n",
    "    \n",
    "    # Train neural network for d\n",
    "    nn.fit(X_train, d_train)\n",
    "    d_pred = nn.predict(X_test)\n",
    "    d_residuals[test_index] = d_test - d_pred\n",
    "\n",
    "# Run linear regression of residualized y on d\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(d_residuals.reshape(-1, 1), y_residuals)\n",
    "print(f\"Coefficient for residualized d on y: {lin_reg.coef_[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052209e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fc37e35",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d10585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "X = nlsy[continuous_var]\n",
    "y = nlsy['lnw_2016']\n",
    "d = nlsy['mom_educCOLL']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Hyperparameter grids\n",
    "rf_params = {'n_estimators': [50, 100], 'max_depth': [None, 5, 10, 20]}\n",
    "ridge_params = {'alpha': [0.1, 1.0, 10.0, 100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4504a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize GridSearchCV for each model\n",
    "rf_grid = GridSearchCV(RandomForestRegressor(random_state=42), rf_params, cv=5)\n",
    "ridge_grid = GridSearchCV(Ridge(), ridge_params, cv=5)\n",
    "\n",
    "# Fit GridSearchCV to find the best Random Forest and Ridge models\n",
    "rf_grid.fit(X, y)\n",
    "ridge_grid.fit(X_scaled, y)\n",
    "\n",
    "# Get the best models\n",
    "best_rf = rf_grid.best_estimator_\n",
    "best_ridge = ridge_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8150acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the best models\n",
    "rf_pred = best_rf.predict(X)\n",
    "ridge_pred = best_ridge.predict(X_scaled)\n",
    "\n",
    "# Ensemble the predictions (average)\n",
    "ensemble_pred = (rf_pred + ridge_pred) / 2\n",
    "\n",
    "# Calculate MSE for each model and the ensemble\n",
    "rf_mse = mean_squared_error(y, rf_pred)\n",
    "ridge_mse = mean_squared_error(y, ridge_pred)\n",
    "ensemble_mse = mean_squared_error(y, ensemble_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdc7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the MSE for each model\n",
    "mse_dict = {'rf': rf_mse, 'ridge': ridge_mse, 'ensemble': ensemble_mse}\n",
    "\n",
    "# Find the model with the minimum MSE\n",
    "best_model = min(mse_dict, key=mse_dict.get)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals for y and d based on the best model\n",
    "if best_model == 'ensemble':\n",
    "    y_residual = y - ensemble_pred\n",
    "    d_residual = d - ensemble_pred  # Assuming d is also a target variable\n",
    "elif best_model == 'rf':\n",
    "    y_residual = y - rf_pred\n",
    "    d_residual = d - rf_pred  # Assuming d is also a target variable\n",
    "else:  # best_model == 'ridge'\n",
    "    y_residual = y - ridge_pred\n",
    "    d_residual = d - ridge_pred  # Assuming d is also a target variable\n",
    "\n",
    "print(f\"Best model based on MSE: {best_model}\")\n",
    "print(f\"Residuals for y and d calculated based on the best model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27ffef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_residual = y_residual.values.reshape(-1,1)\n",
    "d_residual = d_residual.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef48487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the residualized regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2861897",
   "metadata": {},
   "source": [
    "Can you tell the three different uses of K-fold sample splitting in the lecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74ed77",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Set aside a validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ff060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practice!\n",
    "# Hint: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
